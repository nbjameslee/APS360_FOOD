{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet_classification.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TPRW9ZBU5MRM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHwcnJSY5U16","colab_type":"code","outputId":"635663e6-1897-4805-c9e1-1e371a45fbbe","executionInfo":{"status":"ok","timestamp":1563891547431,"user_tz":240,"elapsed":26121,"user":{"displayName":"Food Food","photoUrl":"","userId":"00538189971839826047"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u-F3J5N55aEl","colab_type":"code","colab":{}},"source":["transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QASpmThi5ae8","colab_type":"code","outputId":"80c9afeb-14cb-4556-8b80-e094c02ea6c1","executionInfo":{"status":"ok","timestamp":1563891602859,"user_tz":240,"elapsed":81530,"user":{"displayName":"Food Food","photoUrl":"","userId":"00538189971839826047"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["val_data = torchvision.datasets.ImageFolder('./drive/My Drive/cropped_image/validation',loader = plt.imread,transform=transform)\n","test_data = torchvision.datasets.ImageFolder('./drive/My Drive/cropped_image/test',loader = plt.imread,transform=transform)\n","train_data = torchvision.datasets.ImageFolder('./drive/My Drive/cropped_image/train',loader = plt.imread,transform=transform)\n","\n","total_data = len(val_data) + len(test_data) + len(train_data)\n","\n","print(\"Validation set percentage: \"+str(len(val_data)/total_data))\n","print(\"Test set percentage: \"+str(len(test_data)/total_data))\n","print(\"Training set percentage: \"+str(len(train_data)/total_data))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Validation set percentage: 0.15677253826417764\n","Test set percentage: 0.05258191040726651\n","Training set percentage: 0.7906455513285559\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nC8VYPvOREy0","colab_type":"code","colab":{}},"source":["import torchvision.models\n","#resnet_v1 = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck,[3, 8, 36, 3],num_classes=32).cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9D0nBS1_Ujo1","colab_type":"code","colab":{}},"source":["def get_train_accuracy(model,batch_size=20):\n","    \n","    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,shuffle = True)\n","\n","    correct = 0\n","    total = 0\n","    idx = 0\n","    \n","    for imgs, labels in iter(train_loader):\n","        if idx>20:\n","          break\n","        model = model.cuda()\n","        output = model(imgs.cuda()).cuda() # We don't need to run F.softmax\n","        pred = output.max(1, keepdim=True)[1].cuda() # get the index of the max log-probability\n","        correct += pred.eq(labels.cuda().view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","        idx += 1\n","        \n","    \n","    return correct / total"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iI7yGH7dVser","colab_type":"code","colab":{}},"source":["def get_val_accuracy(model,batch_size=20):\n","    \n","    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle = True)\n","\n","    correct = 0\n","    total = 0\n","    idx = 0\n","    \n","    for imgs, labels in iter(val_loader):\n","        if idx>20:\n","          break\n","        model = model.cuda()\n","        output = model(imgs.cuda()).cuda() # We don't need to run F.softmax\n","        pred = output.max(1, keepdim=True)[1].cuda() # get the index of the max log-probability\n","        correct += pred.eq(labels.cuda().view_as(pred)).sum().item()\n","        total += imgs.shape[0]\n","        idx += 1\n","\n","    return correct / total"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m82jeFDYV7Aq","colab_type":"code","colab":{}},"source":["def get_val_loss(model,batch_size=20):  \n","\n","  val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle = True)\n","  losses = []\n","  criterion = nn.CrossEntropyLoss().cuda()\n","  for imgs, labels in iter(val_loader):\n","      model = model.cuda()\n","      output = model(imgs.cuda()).cuda() # We don't need to run F.softmax\n","      loss = criterion(output, labels.cuda())\n","      losses.append(float(loss)/batch_size)\n","      break\n","\n","  return sum(losses)/len(losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4goaojipSkKp","colab_type":"code","colab":{}},"source":["def train_net(model, batch_size=20, num_epochs=100, learning_rate = 0.0006,weight_decay=0.0002):\n","    checkpoint_num = 0\n","    model = model.cuda()\n","    model = model.train()\n","    start_time = time.time()\n","    \n","    torch.manual_seed(1000)\n","    train_loader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=batch_size)\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay = weight_decay)\n","    \n","\n","    saved_x, saved_train_losses, saved_val_losses, train_acc, val_acc = [], [], [], [], []\n","\n","    # training\n","    \n","    \n","    saved_idx = 0\n","    \n","    for epoch in range(num_epochs):\n","        idx = 0\n","        for imgs, labels in iter(train_loader):\n","            \n","            out = model(imgs.cuda()).cuda()             # forward pass\n","            loss = criterion(out, labels.cuda()) # compute the total loss\n","\n","\n","            loss.backward()               # backward pass (compute parameter updates)\n","            optimizer.step()              # make the updates for each parameter\n","            optimizer.zero_grad()         # a clean up step for PyTorch\n","            print ('processing iterration '+str(idx)+'...')\n","            idx += 1\n","            if idx%200==0:\n","              saved_x.append(saved_idx)\n","              saved_idx += 1\n","              saved_val_losses.append(get_val_loss(model,batch_size))\n","              saved_train_losses.append(float(loss)/batch_size)\n","              print((\"\\tTrain loss: {} | Validation loss: {}\\n\").format(saved_train_losses[-1],saved_val_losses[-1]))\n","\n","        \n","              train_acc.append(get_train_accuracy(model,batch_size=batch_size)) # compute training accuracy \n","              val_acc.append(get_val_accuracy(model,batch_size=batch_size))  # compute validation accuracy\n","\n","\n","              print((\"Epoch {}: Train acc: {} |\"+\"Validation acc: {}\").format(epoch + 1,train_acc[-1],val_acc[-1]))\n","              \n","              model_path = \"bs{0}_lr{1}_epoch{2}_checkpoint_{3}\".format(batch_size,learning_rate,epoch,checkpoint_num)\n","              torch.save({\n","                  'checkpoint_num': checkpoint_num,\n","                  'epoch': epoch,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  #'train_loss': train_losses,\n","                  #'val_loss':val_losses,\n","                  'saved_train_loss': saved_train_losses,\n","                  'saved_val_loss':saved_val_losses,\n","                  'saved_x':saved_x,\n","                  #'iters':iters,\n","                  'train_acc':train_acc,\n","                  'val_acc':val_acc\n","                  }, './drive/My Drive/resnet_model_saved/'+model_path+'.pth')\n","\n","\n","    # plotting\n","    plt.title(\"Training Curve\")\n","    plt.plot(saved_x, saved_train_losses, label=\"Train\")\n","    plt.plot(saved_x, saved_val_losses, label='Validation')\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(saved_x, train_acc, label=\"Train\")\n","    plt.plot(saved_x, val_acc, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Training Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","    \n","    end_time = time.time()\n","    duration = end_time - start_time\n","\n","    print(\"Final Training Accuracy: {}\".format(saved_train_acc[-1]))\n","    print(\"Final Validation Accuracy: {}\".format(saved_val_acc[-1]))\n","    print (\"Trained Duration: {} seconds\".format(duration))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWpI_sb2JRIi","colab_type":"code","colab":{}},"source":["def train_net_continue(model, batch_size=20, num_epochs=100, learning_rate = 0.0006,weight_decay=0.0002):\n","    \n","    model_path = \"bs{0}_lr{1}_epoch{2}_checkpoint_{3}\".format(batch_size,learning_rate,0,0)\n","    checkpoint = torch.load('./drive/My Drive/resnet_model_saved/'+model_path+'.pth')\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    checkpoint_num = checkpoint['checkpoint_num']\n","    model = model.cuda()\n","    model = model.train()\n","    start_time = time.time()\n","    \n","    torch.manual_seed(1000)\n","    train_loader = torch.utils.data.DataLoader(train_data, shuffle = True, batch_size=batch_size)\n","    criterion = nn.CrossEntropyLoss().cuda()\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate,weight_decay = weight_decay)\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    \n","    \n","\n","    saved_x = checkpoint['saved_x']\n","    saved_train_losses = checkpoint['saved_train_loss']\n","    saved_val_losses = checkpoint['saved_val_loss']\n","    train_acc = checkpoint['train_acc']\n","    val_acc = checkpoint['val_acc']\n","    \n","    # training\n","    \n","    \n","    saved_idx = saved_x[-1]+1\n","    \n","    for epoch in range(num_epochs):\n","        idx = 0\n","        for imgs, labels in iter(train_loader):\n","            \n","            out = model(imgs.cuda()).cuda()             # forward pass\n","            loss = criterion(out, labels.cuda()) # compute the total loss\n","\n","\n","            loss.backward()               # backward pass (compute parameter updates)\n","            optimizer.step()              # make the updates for each parameter\n","            optimizer.zero_grad()         # a clean up step for PyTorch\n","            print ('processing iterration '+str(idx)+'...')\n","            idx += 1\n","            if idx%200==0:\n","              saved_x.append(saved_idx)\n","              saved_idx += 1\n","              saved_val_losses.append(get_val_loss(model,batch_size))\n","              saved_train_losses.append(float(loss)/batch_size)\n","              print((\"\\tTrain loss: {} | Validation loss: {}\\n\").format(saved_train_losses[-1],saved_val_losses[-1]))\n","\n","        \n","              train_acc.append(get_train_accuracy(model,batch_size=batch_size)) # compute training accuracy \n","              val_acc.append(get_val_accuracy(model,batch_size=batch_size))  # compute validation accuracy\n","\n","\n","              print((\"Epoch {}: Train acc: {} |\"+\"Validation acc: {}\").format(epoch + 1,train_acc[-1],val_acc[-1]))\n","              checkpoint_num += 1\n","              model_path = \"bs{0}_lr{1}_epoch{2}_checkpoint_{3}\".format(batch_size,learning_rate,epoch,checkpoint_num)\n","              torch.save({\n","                  'checkpoint_num': checkpoint_num,\n","                  'epoch': epoch,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  #'train_loss': train_losses,\n","                  #'val_loss':val_losses,\n","                  'saved_train_loss': saved_train_losses,\n","                  'saved_val_loss':saved_val_losses,\n","                  'saved_x':saved_x,\n","                  #'iters':iters,\n","                  'train_acc':train_acc,\n","                  'val_acc':val_acc\n","                  }, './drive/My Drive/resnet_model_saved/'+model_path+'.pth')\n","\n","\n","    # plotting\n","    plt.title(\"Training Curve\")\n","    plt.plot(saved_x, saved_train_losses, label=\"Train\")\n","    plt.plot(saved_x, saved_val_losses, label='Validation')\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend(loc='best')\n","    plt.show()\n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(saved_x, train_acc, label=\"Train\")\n","    plt.plot(saved_x, val_acc, label=\"Validation\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Training Accuracy\")\n","    plt.legend(loc='best')\n","    plt.show()\n","    \n","    end_time = time.time()\n","    duration = end_time - start_time\n","\n","    print(\"Final Training Accuracy: {}\".format(saved_train_acc[-1]))\n","    print(\"Final Validation Accuracy: {}\".format(saved_val_acc[-1]))\n","    print (\"Trained Duration: {} seconds\".format(duration))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiWNBY4FWZ8l","colab_type":"code","colab":{}},"source":["#res50_v1 = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck,[3, 4, 6, 3],num_classes=32).cuda()\n","#train_net(res50_v1,batch_size=20, num_epochs=100, learning_rate = 0.006,weight_decay=0.0002)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaDZpHRUK2G3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"b7650656-ed9f-4776-a428-8c39319c5c91"},"source":["res50_v1 = torchvision.models.resnet.ResNet(torchvision.models.resnet.Bottleneck,[3, 4, 6, 3],num_classes=32).cuda()\n","train_net_continue(res50_v1,batch_size=20, num_epochs=100, learning_rate = 0.006,weight_decay=0.0002)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["processing iterration 0...\n","processing iterration 1...\n","processing iterration 2...\n","processing iterration 3...\n","processing iterration 4...\n","processing iterration 5...\n","processing iterration 6...\n","processing iterration 7...\n","processing iterration 8...\n","processing iterration 9...\n","processing iterration 10...\n","processing iterration 11...\n","processing iterration 12...\n","processing iterration 13...\n","processing iterration 14...\n","processing iterration 15...\n","processing iterration 16...\n","processing iterration 17...\n","processing iterration 18...\n","processing iterration 19...\n","processing iterration 20...\n","processing iterration 21...\n","processing iterration 22...\n","processing iterration 23...\n","processing iterration 24...\n","processing iterration 25...\n","processing iterration 26...\n","processing iterration 27...\n","processing iterration 28...\n","processing iterration 29...\n","processing iterration 30...\n","processing iterration 31...\n","processing iterration 32...\n","processing iterration 33...\n","processing iterration 34...\n","processing iterration 35...\n","processing iterration 36...\n","processing iterration 37...\n","processing iterration 38...\n","processing iterration 39...\n","processing iterration 40...\n","processing iterration 41...\n","processing iterration 42...\n","processing iterration 43...\n","processing iterration 44...\n","processing iterration 45...\n","processing iterration 46...\n","processing iterration 47...\n","processing iterration 48...\n","processing iterration 49...\n","processing iterration 50...\n","processing iterration 51...\n","processing iterration 52...\n","processing iterration 53...\n","processing iterration 54...\n","processing iterration 55...\n","processing iterration 56...\n","processing iterration 57...\n","processing iterration 58...\n","processing iterration 59...\n"],"name":"stdout"}]}]}